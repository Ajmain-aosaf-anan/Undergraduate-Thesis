{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers torch sympy matplotlib opencv-python numpy diffusers torch torchvision\n",
    "%pip install openai gym[pybullet] manim pydantic requests accelerate\n",
    "%pip install pytorch-lightning scikit-learn datasets nltk sentencepiece\n",
    "\n",
    "%pip install transformers torch torchvision torchaudio diffusers accelerate\n",
    "%pip install opencv-python numpy matplotlib scikit-image rouge-score\n",
    "%pip install torchsummary pytorch-fid\n",
    "%pip install gym[pybullet] manim openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import json\n",
    "\n",
    "class ProblemClassifier:\n",
    "    def __init__(self, model_path=\"bert-base-uncased\"):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "        self.model = BertForSequenceClassification.from_pretrained(model_path, num_labels=3)  # Easy, Medium, Hard\n",
    "        self.label_map = {\"Easy\": 0, \"Medium\": 1, \"Hard\": 2}  # Mapping difficulty to labels\n",
    "\n",
    "    def classify_problem(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "        return list(self.label_map.keys())[prediction]\n",
    "\n",
    "# Load the dataset\n",
    "with open(\"updated_physics_problems_dataset_2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# Preprocess dataset for classification\n",
    "classifier = ProblemClassifier()\n",
    "for problem in dataset:\n",
    "    input_text = problem[\"Problem_Statement\"] + \" Concepts: \" + problem[\"Concepts_Covered\"]\n",
    "    predicted_difficulty = classifier.classify_problem(input_text)\n",
    "    print(f\"Problem ID: {problem['Problem_ID']}, Predicted Difficulty: {predicted_difficulty}, Actual: {problem['Difficulty_Level']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import json\n",
    "\n",
    "# Load dataset\n",
    "with open(\"updated_physics_problems_dataset_2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# Map difficulty levels to numerical labels\n",
    "label_map = {\"Easy\": 0, \"Medium\": 1, \"Hard\": 2}\n",
    "\n",
    "def preprocess_data(data):\n",
    "    processed_data = []\n",
    "    for problem in data:\n",
    "        input_text = problem[\"Problem_Statement\"] + \" Concepts: \" + problem[\"Concepts_Covered\"]\n",
    "        label = label_map[problem[\"Difficulty_Level\"]]\n",
    "        processed_data.append({\"text\": input_text, \"label\": label})\n",
    "    return processed_data\n",
    "\n",
    "# Preprocess dataset\n",
    "data = preprocess_data(dataset)\n",
    "dataset = Dataset.from_list(data)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\")\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Load model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_finetuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    eval_dataset=tokenized_datasets,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save fine-tuned model\n",
    "model.save_pretrained(\"./bert_finetuned\")\n",
    "tokenizer.save_pretrained(\"./bert_finetuned\")\n",
    "\n",
    "print(\"Fine-tuning complete. Model saved to ./bert_finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "def load_finetuned_model(model_path=\"./bert_finetuned\"):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "    return tokenizer, model\n",
    "\n",
    "def classify_problem(problem_statement, concepts, tokenizer, model):\n",
    "    input_text = problem_statement + \" Concepts: \" + concepts\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "    label_map = {0: \"Easy\", 1: \"Medium\", 2: \"Hard\"}\n",
    "    return label_map[prediction]\n",
    "\n",
    "# Load the fine-tuned model\n",
    "tokenizer, model = load_finetuned_model()\n",
    "\n",
    "# Example problem to test\n",
    "example_problem = \"A ball is thrown vertically upward with an initial velocity of 20 meters per second. Calculate the maximum height reached.\"\n",
    "example_concepts = \"Kinematics, free fall, acceleration due to gravity.\"\n",
    "\n",
    "# Classify difficulty\n",
    "predicted_difficulty = classify_problem(example_problem, example_concepts, tokenizer, model)\n",
    "print(f\"Predicted Difficulty: {predicted_difficulty}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import json\n",
    "\n",
    "# Load the fine-tuned model\n",
    "def load_finetuned_model(model_path=\"./bert_finetuned\"):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "    return tokenizer, model\n",
    "\n",
    "def classify_problem(problem_statement, concepts, tokenizer, model):\n",
    "    input_text = problem_statement + \" Concepts: \" + concepts\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "    return prediction\n",
    "\n",
    "# Load dataset for evaluation\n",
    "with open(\"updated_physics_problems_dataset_2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "label_map = {\"Easy\": 0, \"Medium\": 1, \"Hard\": 2}\n",
    "reverse_label_map = {0: \"Easy\", 1: \"Medium\", 2: \"Hard\"}\n",
    "\n",
    "tokenizer, model = load_finetuned_model()\n",
    "\n",
    "# Evaluate model\n",
    "correct = 0\n",
    "total = len(dataset)\n",
    "\n",
    "for problem in dataset:\n",
    "    actual_label = label_map[problem[\"Difficulty_Level\"]]\n",
    "    predicted_label = classify_problem(problem[\"Problem_Statement\"], problem[\"Concepts_Covered\"], tokenizer, model)\n",
    "    if actual_label == predicted_label:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = (correct / total) * 100\n",
    "print(f\"Model Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import json\n",
    "\n",
    "# Load the fine-tuned model\n",
    "def load_finetuned_model(model_path=\"./bert_finetuned\"):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "    return tokenizer, model\n",
    "\n",
    "def classify_problem(problem_statement, concepts, tokenizer, model):\n",
    "    input_text = problem_statement + \" Concepts: \" + concepts\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "    return prediction\n",
    "\n",
    "# Load dataset for evaluation\n",
    "with open(\"updated_physics_problems_dataset_2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "label_map = {\"Easy\": 0, \"Medium\": 1, \"Hard\": 2}\n",
    "reverse_label_map = {0: \"Easy\", 1: \"Medium\", 2: \"Hard\"}\n",
    "\n",
    "tokenizer, model = load_finetuned_model()\n",
    "\n",
    "# Evaluate model\n",
    "correct = 0\n",
    "total = len(dataset)\n",
    "\n",
    "for problem in dataset:\n",
    "    actual_label = label_map[problem[\"Difficulty_Level\"]]\n",
    "    predicted_label = classify_problem(problem[\"Problem_Statement\"], problem[\"Concepts_Covered\"], tokenizer, model)\n",
    "    if actual_label == predicted_label:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = (correct / total) * 100\n",
    "print(f\"Model Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Load the fine-tuned model\n",
    "def load_finetuned_model(model_path=\"./bert_finetuned\"):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "    return tokenizer, model\n",
    "\n",
    "def classify_problem(problem_statement, concepts, tokenizer, model):\n",
    "    input_text = problem_statement + \" Concepts: \" + concepts\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "    return prediction\n",
    "\n",
    "# Load dataset for evaluation\n",
    "with open(\"updated_physics_problems_dataset_2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "label_map = {\"Easy\": 0, \"Medium\": 1, \"Hard\": 2}\n",
    "reverse_label_map = {0: \"Easy\", 1: \"Medium\", 2: \"Hard\"}\n",
    "\n",
    "tokenizer, model = load_finetuned_model()\n",
    "\n",
    "# Collect true and predicted labels\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for problem in dataset:\n",
    "    actual_label = label_map[problem[\"Difficulty_Level\"]]\n",
    "    predicted_label = classify_problem(problem[\"Problem_Statement\"], problem[\"Concepts_Covered\"], tokenizer, model)\n",
    "    y_true.append(actual_label)\n",
    "    y_pred.append(predicted_label)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "labels = [\"Easy\", \"Medium\", \"Hard\"]\n",
    "\n",
    "def plot_confusion_matrix(cm, labels):\n",
    "    plt.figure(figsize=(6,6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "# Display results\n",
    "plot_confusion_matrix(cm, labels)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import pandas as pd\n",
    "\n",
    "# Load the fine-tuned model\n",
    "def load_finetuned_model(model_path=\"./bert_finetuned\"):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "    return tokenizer, model\n",
    "\n",
    "def classify_problem(problem_statement, concepts, tokenizer, model):\n",
    "    input_text = problem_statement + \" Concepts: \" + concepts\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "    return prediction\n",
    "\n",
    "# Load dataset for evaluation\n",
    "with open(\"updated_physics_problems_dataset_2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "label_map = {\"Easy\": 0, \"Medium\": 1, \"Hard\": 2}\n",
    "reverse_label_map = {0: \"Easy\", 1: \"Medium\", 2: \"Hard\"}\n",
    "\n",
    "tokenizer, model = load_finetuned_model()\n",
    "\n",
    "# Collect true and predicted labels\n",
    "misclassified = []\n",
    "\n",
    "for problem in dataset:\n",
    "    actual_label = label_map[problem[\"Difficulty_Level\"]]\n",
    "    predicted_label = classify_problem(problem[\"Problem_Statement\"], problem[\"Concepts_Covered\"], tokenizer, model)\n",
    "    if actual_label != predicted_label:\n",
    "        misclassified.append({\n",
    "            \"Problem_ID\": problem[\"Problem_ID\"],\n",
    "            \"Problem_Statement\": problem[\"Problem_Statement\"],\n",
    "            \"Concepts_Covered\": problem[\"Concepts_Covered\"],\n",
    "            \"Actual Difficulty\": reverse_label_map[actual_label],\n",
    "            \"Predicted Difficulty\": reverse_label_map[predicted_label]\n",
    "        })\n",
    "\n",
    "# Convert misclassifications to a DataFrame and display\n",
    "misclassified_df = pd.DataFrame(misclassified)\n",
    "print(\"Misclassified Problems:\")\n",
    "print(misclassified_df)\n",
    "misclassified_df.to_csv(\"misclassified_problems.csv\", index=False)\n",
    "print(\"Misclassification analysis saved to misclassified_problems.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load FLAN-T5 model and tokenizer\n",
    "def load_flan_t5(model_path=\"google/flan-t5-large\"):\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "    return tokenizer, model\n",
    "\n",
    "def generate_solution(problem_statement, concepts, tokenizer, model, max_length=200):\n",
    "    input_text = f\"Solve step-by-step: {problem_statement} Concepts: {concepts}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_length=max_length)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Load dataset\n",
    "with open(\"updated_physics_problems_dataset_2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "tokenizer, model = load_flan_t5()\n",
    "\n",
    "# Generate solutions for each problem\n",
    "solutions = []\n",
    "\n",
    "for problem in dataset:\n",
    "    problem_id = problem[\"Problem_ID\"]\n",
    "    problem_statement = problem[\"Problem_Statement\"]\n",
    "    concepts = problem[\"Concepts_Covered\"]\n",
    "    generated_solution = generate_solution(problem_statement, concepts, tokenizer, model)\n",
    "    solutions.append({\n",
    "        \"Problem_ID\": problem_id,\n",
    "        \"Problem_Statement\": problem_statement,\n",
    "        \"Generated_Solution\": generated_solution,\n",
    "        \"Final_Answer\": problem.get(\"Final_Answer\", \"N/A\")\n",
    "    })\n",
    "\n",
    "# Save results\n",
    "solutions_df = pd.DataFrame(solutions)\n",
    "solutions_df.to_csv(\"flan_t5_generated_solutions.csv\", index=False)\n",
    "print(\"Generated solutions saved to flan_t5_generated_solutions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Load generated solutions and actual answers\n",
    "df = pd.read_csv(\"flan_t5_generated_solutions.csv\")\n",
    "\n",
    "def compute_rouge(reference, prediction):\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "    scores = scorer.score(reference, prediction)\n",
    "    return scores[\"rouge1\"].fmeasure, scores[\"rouge2\"].fmeasure, scores[\"rougeL\"].fmeasure\n",
    "\n",
    "# Evaluate generated solutions\n",
    "rouge1_scores, rouge2_scores, rougeL_scores = [], [], []\n",
    "for _, row in df.iterrows():\n",
    "    reference = str(row[\"Final_Answer\"])\n",
    "    prediction = str(row[\"Generated_Solution\"])\n",
    "    r1, r2, rL = compute_rouge(reference, prediction)\n",
    "    rouge1_scores.append(r1)\n",
    "    rouge2_scores.append(r2)\n",
    "    rougeL_scores.append(rL)\n",
    "\n",
    "df[\"ROUGE-1\"] = rouge1_scores\n",
    "df[\"ROUGE-2\"] = rouge2_scores\n",
    "df[\"ROUGE-L\"] = rougeL_scores\n",
    "\n",
    "df.to_csv(\"flan_t5_evaluation_results.csv\", index=False)\n",
    "print(\"Evaluation complete. Results saved to flan_t5_evaluation_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import json\n",
    "\n",
    "# Load dataset\n",
    "with open(\"updated_physics_problems_dataset_2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# Prepare dataset for fine-tuning\n",
    "def preprocess_data(data):\n",
    "    processed_data = []\n",
    "    for problem in data:\n",
    "        input_text = f\"Solve step-by-step: {problem['Problem_Statement']} Concepts: {problem['Concepts_Covered']}\"\n",
    "        target_text = problem.get(\"Final_Answer\", \"\")\n",
    "        processed_data.append({\"input_text\": input_text, \"target_text\": target_text})\n",
    "    return processed_data\n",
    "\n",
    "data = preprocess_data(dataset)\n",
    "dataset = Dataset.from_list(data)\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(examples[\"input_text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "    targets = tokenizer(examples[\"target_text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Load FLAN-T5 model\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./flan_t5_finetuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    eval_dataset=tokenized_datasets,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save fine-tuned model\n",
    "model.save_pretrained(\"./flan_t5_finetuned\")\n",
    "tokenizer.save_pretrained(\"./flan_t5_finetuned\")\n",
    "\n",
    "print(\"Fine-tuning complete. Model saved to ./flan_t5_finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load fine-tuned FLAN-T5 model\n",
    "def load_finetuned_model(model_path=\"./flan_t5_finetuned\"):\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "    return tokenizer, model\n",
    "\n",
    "def generate_solution(problem_statement, concepts, tokenizer, model, max_length=200):\n",
    "    input_text = f\"Solve step-by-step: {problem_statement} Concepts: {concepts}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_length=max_length)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Load the fine-tuned model\n",
    "tokenizer, model = load_finetuned_model()\n",
    "\n",
    "# Example test problem\n",
    "example_problem = \"A ball is thrown vertically upward with an initial velocity of 20 meters per second. Calculate the maximum height reached.\"\n",
    "example_concepts = \"Kinematics, free fall, acceleration due to gravity.\"\n",
    "\n",
    "# Generate solution\n",
    "generated_solution = generate_solution(example_problem, example_concepts, tokenizer, model)\n",
    "print(\"Generated Solution:\")\n",
    "print(generated_solution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load fine-tuned FLAN-T5 model\n",
    "def load_finetuned_model(model_path=\"./flan_t5_finetuned\"):\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "    return tokenizer, model\n",
    "\n",
    "def generate_solution(problem_statement, concepts, tokenizer, model, max_length=200):\n",
    "    input_text = f\"Solve step-by-step: {problem_statement} Concepts: {concepts}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_length=max_length)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Load the fine-tuned model\n",
    "tokenizer, model = load_finetuned_model()\n",
    "\n",
    "# Load dataset for batch testing\n",
    "with open(\"updated_physics_problems_dataset_2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# Generate solutions for all problems\n",
    "solutions = []\n",
    "for problem in dataset:\n",
    "    problem_id = problem[\"Problem_ID\"]\n",
    "    problem_statement = problem[\"Problem_Statement\"]\n",
    "    concepts = problem[\"Concepts_Covered\"]\n",
    "    generated_solution = generate_solution(problem_statement, concepts, tokenizer, model)\n",
    "    solutions.append({\n",
    "        \"Problem_ID\": problem_id,\n",
    "        \"Problem_Statement\": problem_statement,\n",
    "        \"Generated_Solution\": generated_solution,\n",
    "        \"Final_Answer\": problem.get(\"Final_Answer\", \"N/A\")\n",
    "    })\n",
    "\n",
    "# Save results to CSV\n",
    "solutions_df = pd.DataFrame(solutions)\n",
    "solutions_df.to_csv(\"flan_t5_batch_generated_solutions.csv\", index=False)\n",
    "print(\"Batch testing complete. Results saved to flan_t5_batch_generated_solutions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Load batch test results\n",
    "df = pd.read_csv(\"flan_t5_batch_generated_solutions.csv\")\n",
    "\n",
    "def compute_rouge(reference, prediction):\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "    scores = scorer.score(reference, prediction)\n",
    "    return scores[\"rouge1\"].fmeasure, scores[\"rouge2\"].fmeasure, scores[\"rougeL\"].fmeasure\n",
    "\n",
    "# Evaluate generated solutions\n",
    "rouge1_scores, rouge2_scores, rougeL_scores = [], [], []\n",
    "for _, row in df.iterrows():\n",
    "    reference = str(row[\"Final_Answer\"])\n",
    "    prediction = str(row[\"Generated_Solution\"])\n",
    "    r1, r2, rL = compute_rouge(reference, prediction)\n",
    "    rouge1_scores.append(r1)\n",
    "    rouge2_scores.append(r2)\n",
    "    rougeL_scores.append(rL)\n",
    "\n",
    "df[\"ROUGE-1\"] = rouge1_scores\n",
    "df[\"ROUGE-2\"] = rouge2_scores\n",
    "df[\"ROUGE-L\"] = rougeL_scores\n",
    "\n",
    "df.to_csv(\"flan_t5_batch_evaluation_results.csv\", index=False)\n",
    "print(\"Batch evaluation complete. Results saved to flan_t5_batch_evaluation_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load evaluation results\n",
    "df = pd.read_csv(\"flan_t5_batch_evaluation_results.csv\")\n",
    "\n",
    "# Plot ROUGE score distributions\n",
    "def plot_rouge_scores(df):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.histplot(df[\"ROUGE-1\"], bins=20, kde=True, label=\"ROUGE-1\", color='blue')\n",
    "    sns.histplot(df[\"ROUGE-2\"], bins=20, kde=True, label=\"ROUGE-2\", color='green')\n",
    "    sns.histplot(df[\"ROUGE-L\"], bins=20, kde=True, label=\"ROUGE-L\", color='red')\n",
    "    plt.xlabel(\"ROUGE Score\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Distribution of ROUGE Scores\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Call the plotting function\n",
    "plot_rouge_scores(df)\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"Summary of ROUGE Scores:\")\n",
    "print(df[[\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\"]].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load pre-finetuning and post-finetuning evaluation results\n",
    "pre_finetune_df = pd.read_csv(\"flan_t5_batch_evaluation_results_pre_finetune.csv\")\n",
    "post_finetune_df = pd.read_csv(\"flan_t5_batch_evaluation_results.csv\")\n",
    "\n",
    "# Merge datasets for comparison\n",
    "pre_finetune_df[\"Model\"] = \"Pre-Finetuned\"\n",
    "post_finetune_df[\"Model\"] = \"Post-Finetuned\"\n",
    "comparison_df = pd.concat([pre_finetune_df, post_finetune_df])\n",
    "\n",
    "# Plot ROUGE score comparisons\n",
    "def plot_rouge_comparison(df):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x=\"Model\", y=\"ROUGE-1\", data=df, palette=[\"blue\", \"green\"])\n",
    "    plt.title(\"ROUGE-1 Score Comparison Before and After Finetuning\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x=\"Model\", y=\"ROUGE-2\", data=df, palette=[\"blue\", \"green\"])\n",
    "    plt.title(\"ROUGE-2 Score Comparison Before and After Finetuning\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x=\"Model\", y=\"ROUGE-L\", data=df, palette=[\"blue\", \"green\"])\n",
    "    plt.title(\"ROUGE-L Score Comparison Before and After Finetuning\")\n",
    "    plt.show()\n",
    "\n",
    "# Call the comparison function\n",
    "plot_rouge_comparison(comparison_df)\n",
    "\n",
    "# Print summary statistics comparison\n",
    "print(\"Summary of ROUGE Scores Before and After Finetuning:\")\n",
    "print(comparison_df.groupby(\"Model\")[[\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\"]].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "# Load LLaMA model and tokenizer\n",
    "def load_llama_model(model_path=\"meta-llama/Llama-2-7b-chat-hf\"):\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "    model = LlamaForCausalLM.from_pretrained(model_path)\n",
    "    return tokenizer, model\n",
    "\n",
    "# Preprocess the Physics Problem Dataset\n",
    "def preprocess_physics_data(dataset_path=\"updated_physics_problems_dataset_2.json\"):\n",
    "    with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        dataset = json.load(f)\n",
    "    processed_data = []\n",
    "    for problem in dataset:\n",
    "        input_text = f\"Physics Problem: {problem['Problem_Statement']}\\nConcepts: {problem['Concepts_Covered']}\"\n",
    "        processed_data.append({\"input_text\": input_text, \"answer\": problem.get(\"Final_Answer\", \"N/A\")})\n",
    "    return processed_data\n",
    "\n",
    "def generate_tutoring_response(question, context, tokenizer, model, max_length=300):\n",
    "    input_text = f\"Student Question: {question}\\nContext: {context}\\nAI Tutor Response:\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_length=max_length)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Load the LLaMA model\n",
    "tokenizer, model = load_llama_model()\n",
    "\n",
    "# Load and preprocess dataset\n",
    "dataset = preprocess_physics_data()\n",
    "\n",
    "# Example student interaction\n",
    "example_question = \"Can you explain Newton's laws of motion in a simple way?\"\n",
    "example_context = \"Newton's laws describe the relationship between the motion of an object and the forces acting on it.\"\n",
    "\n",
    "# Generate AI tutor response\n",
    "tutor_response = generate_tutoring_response(example_question, example_context, tokenizer, model)\n",
    "print(\"AI Tutor Response:\")\n",
    "print(tutor_response)\n",
    "\n",
    "# Evaluate AI Tutoring Performance (Placeholder for ROUGE/BLEU Scoring)\n",
    "def evaluate_response(reference, generated):\n",
    "    from rouge_score import rouge_scorer\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "    scores = scorer.score(reference, generated)\n",
    "    return scores\n",
    "\n",
    "# Example evaluation\n",
    "if dataset:\n",
    "    reference_answer = dataset[0][\"answer\"]\n",
    "    evaluation_scores = evaluate_response(reference_answer, tutor_response)\n",
    "    print(\"Evaluation Scores:\", evaluation_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "def load_bert_model(model_path=\"bert-base-uncased\"):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path, num_labels=3)  # Easy, Medium, Hard\n",
    "    return tokenizer, model\n",
    "\n",
    "def classify_problem(problem_statement, concepts, tokenizer, model):\n",
    "    input_text = f\"{problem_statement} Concepts: {concepts}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "    label_map = {0: \"Easy\", 1: \"Medium\", 2: \"Hard\"}\n",
    "    return label_map[prediction]\n",
    "\n",
    "def generate_image(prompt, model_path=\"CompVis/stable-diffusion-v1-4\"):\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(model_path)\n",
    "    pipe.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    image = pipe(prompt).images[0]\n",
    "    return image\n",
    "\n",
    "def apply_pix2pix_style(image, style_model_path=\"style_transfer_pix2pix.pth\"):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    input_image = transform(image).unsqueeze(0)\n",
    "    pix2pix_model = torch.jit.load(style_model_path)\n",
    "    pix2pix_model.eval()\n",
    "    with torch.no_grad():\n",
    "        output_image = pix2pix_model(input_image).squeeze().detach().cpu()\n",
    "    output_image = transforms.ToPILImage()(output_image)\n",
    "    return output_image\n",
    "\n",
    "# Load models\n",
    "tokenizer, bert_model = load_bert_model()\n",
    "\n",
    "# Load dataset\n",
    "with open(\"updated_physics_problems_dataset_2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# Process dataset\n",
    "for problem in dataset:\n",
    "    problem_id = problem[\"Problem_ID\"]\n",
    "    problem_statement = problem[\"Problem_Statement\"]\n",
    "    concepts = problem[\"Concepts_Covered\"]\n",
    "    difficulty = classify_problem(problem_statement, concepts, tokenizer, bert_model)\n",
    "    image_prompt = problem[\"3D_Diagram_Description\"]\n",
    "    \n",
    "    print(f\"Generating image for Problem {problem_id} ({difficulty} difficulty)...\")\n",
    "    generated_image = generate_image(image_prompt)\n",
    "    \n",
    "    print(\"Applying style transfer...\")\n",
    "    styled_image = apply_pix2pix_style(generated_image)\n",
    "    \n",
    "    styled_image.save(f\"generated_images/problem_{problem_id}_styled.png\")\n",
    "    print(f\"Image saved: generated_images/problem_{problem_id}_styled.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def load_images_from_folder(folder_path, num_images=9):\n",
    "    images = []\n",
    "    filenames = sorted(os.listdir(folder_path))[:num_images]\n",
    "    for filename in filenames:\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        img = Image.open(img_path)\n",
    "        images.append((filename, img))\n",
    "    return images\n",
    "\n",
    "def plot_image_grid(images, grid_size=(3, 3)):\n",
    "    fig, axes = plt.subplots(grid_size[0], grid_size[1], figsize=(10, 10))\n",
    "    fig.suptitle(\"Generated Physics Problem Illustrations\", fontsize=16)\n",
    "    for ax, (filename, img) in zip(axes.flat, images):\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(filename)\n",
    "        ax.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Load and display images\n",
    "image_folder = \"generated_images\"\n",
    "images = load_images_from_folder(image_folder, num_images=9)\n",
    "plot_image_grid(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Load BERT model for classification\n",
    "def load_bert_model(model_path=\"bert-base-uncased\"):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path, num_labels=3)  # Easy, Medium, Hard\n",
    "    return tokenizer, model\n",
    "\n",
    "def classify_problem(problem_statement, concepts, tokenizer, model):\n",
    "    input_text = f\"{problem_statement} Concepts: {concepts}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "    label_map = {0: \"Easy\", 1: \"Medium\", 2: \"Hard\"}\n",
    "    return label_map[prediction]\n",
    "\n",
    "def generate_image(prompt, model_path=\"CompVis/stable-diffusion-v1-4\"):\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(model_path)\n",
    "    pipe.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    image = pipe(prompt).images[0]\n",
    "    return image\n",
    "\n",
    "def save_image(image, path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    image.save(path)\n",
    "\n",
    "def process_problem(problem, tokenizer, bert_model, model_path=\"CompVis/stable-diffusion-v1-4\"):\n",
    "    problem_id = problem[\"Problem_ID\"]\n",
    "    problem_statement = problem[\"Problem_Statement\"]\n",
    "    concepts = problem[\"Concepts_Covered\"]\n",
    "    difficulty = classify_problem(problem_statement, concepts, tokenizer, bert_model)\n",
    "    image_prompt = problem[\"3D_Diagram_Description\"]\n",
    "    \n",
    "    print(f\"Generating image for Problem {problem_id} ({difficulty} difficulty)...\")\n",
    "    generated_image = generate_image(image_prompt, model_path)\n",
    "    save_image(generated_image, f\"generated_images/problem_{problem_id}.png\")\n",
    "    print(f\"Image saved: generated_images/problem_{problem_id}.png\")\n",
    "\n",
    "# Load models\n",
    "tokenizer, bert_model = load_bert_model()\n",
    "\n",
    "# Load dataset\n",
    "with open(\"updated_physics_problems_dataset_2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# Generate images in parallel\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    for problem in dataset:\n",
    "        executor.submit(process_problem, problem, tokenizer, bert_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(filename='image_generation.log', level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Load BERT model for classification\n",
    "def load_bert_model(model_path=\"bert-base-uncased\"):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path, num_labels=3)  # Easy, Medium, Hard\n",
    "    return tokenizer, model\n",
    "\n",
    "def classify_problem(problem_statement, concepts, tokenizer, model):\n",
    "    input_text = f\"{problem_statement} Concepts: {concepts}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "    label_map = {0: \"Easy\", 1: \"Medium\", 2: \"Hard\"}\n",
    "    return label_map[prediction]\n",
    "\n",
    "def generate_image(prompt, model_path=\"CompVis/stable-diffusion-v1-4\"):\n",
    "    try:\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(model_path)\n",
    "        pipe.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        image = pipe(prompt).images[0]\n",
    "        return image\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error generating image: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def save_image(image, path):\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        image.save(path)\n",
    "        logging.info(f\"Image saved: {path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving image: {str(e)}\")\n",
    "\n",
    "def process_problem(problem, tokenizer, bert_model, model_path=\"CompVis/stable-diffusion-v1-4\"):\n",
    "    try:\n",
    "        problem_id = problem[\"Problem_ID\"]\n",
    "        problem_statement = problem[\"Problem_Statement\"]\n",
    "        concepts = problem[\"Concepts_Covered\"]\n",
    "        difficulty = classify_problem(problem_statement, concepts, tokenizer, bert_model)\n",
    "        image_prompt = problem[\"3D_Diagram_Description\"]\n",
    "        \n",
    "        logging.info(f\"Generating image for Problem {problem_id} ({difficulty} difficulty)...\")\n",
    "        generated_image = generate_image(image_prompt, model_path)\n",
    "        if generated_image:\n",
    "            save_image(generated_image, f\"generated_images/problem_{problem_id}.png\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing problem {problem_id}: {str(e)}\")\n",
    "\n",
    "# Load models\n",
    "tokenizer, bert_model = load_bert_model()\n",
    "\n",
    "# Load dataset\n",
    "with open(\"updated_physics_problems_dataset_2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# Generate images in parallel\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    for problem in dataset:\n",
    "        executor.submit(process_problem, problem, tokenizer, bert_model)\n",
    "\n",
    "logging.info(\"Batch image generation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# Load CLIP model for evaluation\n",
    "def load_clip_model():\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    return model, processor\n",
    "\n",
    "def evaluate_image(image_path, prompt, model, processor):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(text=[prompt], images=image, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    score = outputs.logits_per_image.item()\n",
    "    return score\n",
    "\n",
    "# Load generated images and dataset\n",
    "dataset_path = \"updated_physics_problems_dataset_2.json\"\n",
    "image_folder = \"generated_images\"\n",
    "\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, processor = load_clip_model()\n",
    "model.to(torch_device)\n",
    "\n",
    "with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = pd.read_json(f)\n",
    "\n",
    "evaluation_results = []\n",
    "for problem in dataset.itertuples():\n",
    "    problem_id = problem.Problem_ID\n",
    "    image_path = os.path.join(image_folder, f\"problem_{problem_id}.png\")\n",
    "    prompt = problem._asdict().get(\"3D_Diagram_Description\", \"\")\n",
    "    \n",
    "    if os.path.exists(image_path):\n",
    "        score = evaluate_image(image_path, prompt, model, processor)\n",
    "        evaluation_results.append({\"Problem_ID\": problem_id, \"Image_Path\": image_path, \"Score\": score})\n",
    "\n",
    "# Save results\n",
    "evaluation_df = pd.DataFrame(evaluation_results)\n",
    "evaluation_df.to_csv(\"image_evaluation_results.csv\", index=False)\n",
    "print(\"Evaluation complete. Results saved to image_evaluation_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load evaluation results\n",
    "df = pd.read_csv(\"image_evaluation_results.csv\")\n",
    "\n",
    "# Plot distribution of CLIP similarity scores\n",
    "def plot_evaluation_scores(df):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(df[\"Score\"], bins=20, kde=True, color='blue')\n",
    "    plt.xlabel(\"CLIP Similarity Score\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Distribution of Image Evaluation Scores\")\n",
    "    plt.show()\n",
    "\n",
    "# Scatter plot of Problem IDs vs Scores\n",
    "def plot_problem_scores(df):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.scatterplot(x=df[\"Problem_ID\"], y=df[\"Score\"], hue=df[\"Score\"], palette=\"viridis\", size=df[\"Score\"], sizes=(20, 200))\n",
    "    plt.xlabel(\"Problem ID\")\n",
    "    plt.ylabel(\"CLIP Similarity Score\")\n",
    "    plt.title(\"Evaluation Scores Across Problems\")\n",
    "    plt.legend(title=\"Score\", bbox_to_anchor=(1, 1))\n",
    "    plt.show()\n",
    "\n",
    "# Run plots\n",
    "plot_evaluation_scores(df)\n",
    "plot_problem_scores(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load evaluation results\n",
    "df = pd.read_csv(\"image_evaluation_results.csv\")\n",
    "\n",
    "# Generate summary statistics\n",
    "summary = df[\"Score\"].describe()\n",
    "\n",
    "# Identify best and worst performing images\n",
    "best_images = df.nlargest(5, \"Score\")\n",
    "worst_images = df.nsmallest(5, \"Score\")\n",
    "\n",
    "# Save report to a text file\n",
    "report_path = \"image_evaluation_report.txt\"\n",
    "with open(report_path, \"w\") as f:\n",
    "    f.write(\"Image Evaluation Report\\n\")\n",
    "    f.write(\"========================\\n\\n\")\n",
    "    f.write(\"Summary Statistics:\\n\")\n",
    "    f.write(str(summary) + \"\\n\\n\")\n",
    "    f.write(\"Top 5 Best Performing Images:\\n\")\n",
    "    f.write(best_images.to_string(index=False) + \"\\n\\n\")\n",
    "    f.write(\"Top 5 Worst Performing Images:\\n\")\n",
    "    f.write(worst_images.to_string(index=False) + \"\\n\")\n",
    "\n",
    "print(f\"Report generated and saved to {report_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Load CycleGAN model for refinement\n",
    "def load_cyclegan_model(model_path=\"cyclegan_refinement.pth\"):\n",
    "    model = torch.jit.load(model_path)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Load ControlNet for structured enhancement\n",
    "def load_controlnet_model(model_path=\"controlnet.pth\"):\n",
    "    model = torch.jit.load(model_path)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Apply CycleGAN for refinement\n",
    "def apply_cyclegan(image, model):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    input_image = transform(image).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        refined_image = model(input_image).squeeze().detach().cpu()\n",
    "    refined_image = transforms.ToPILImage()(refined_image)\n",
    "    return refined_image\n",
    "\n",
    "# Apply ControlNet for structured enhancement\n",
    "def apply_controlnet(image, model):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    input_image = transform(image).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        enhanced_image = model(input_image).squeeze().detach().cpu()\n",
    "    enhanced_image = transforms.ToPILImage()(enhanced_image)\n",
    "    return enhanced_image\n",
    "\n",
    "# Load models\n",
    "cyclegan_model = load_cyclegan_model()\n",
    "controlnet_model = load_controlnet_model()\n",
    "\n",
    "# Process images\n",
    "image_folder = \"generated_images\"\n",
    "refined_folder = \"refined_images\"\n",
    "os.makedirs(refined_folder, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(image_folder):\n",
    "    image_path = os.path.join(image_folder, filename)\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    print(f\"Refining {filename} with CycleGAN...\")\n",
    "    refined_image = apply_cyclegan(image, cyclegan_model)\n",
    "    \n",
    "    print(f\"Enhancing {filename} with ControlNet...\")\n",
    "    final_image = apply_controlnet(refined_image, controlnet_model)\n",
    "    \n",
    "    final_image.save(os.path.join(refined_folder, filename))\n",
    "    print(f\"Saved refined image: {os.path.join(refined_folder, filename)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def load_images(image_folder, refined_folder, num_images=5):\n",
    "    image_pairs = []\n",
    "    filenames = sorted(os.listdir(image_folder))[:num_images]\n",
    "    for filename in filenames:\n",
    "        orig_path = os.path.join(image_folder, filename)\n",
    "        refined_path = os.path.join(refined_folder, filename)\n",
    "        if os.path.exists(refined_path):\n",
    "            orig_image = Image.open(orig_path).convert(\"RGB\")\n",
    "            refined_image = Image.open(refined_path).convert(\"RGB\")\n",
    "            image_pairs.append((filename, orig_image, refined_image))\n",
    "    return image_pairs\n",
    "\n",
    "def plot_comparison(image_pairs):\n",
    "    fig, axes = plt.subplots(len(image_pairs), 2, figsize=(10, 5 * len(image_pairs)))\n",
    "    fig.suptitle(\"Original vs. Refined Images\", fontsize=16)\n",
    "    \n",
    "    for ax_row, (filename, orig_img, refined_img) in zip(axes, image_pairs):\n",
    "        ax_row[0].imshow(orig_img)\n",
    "        ax_row[0].set_title(f\"Original - {filename}\")\n",
    "        ax_row[0].axis(\"off\")\n",
    "        \n",
    "        ax_row[1].imshow(refined_img)\n",
    "        ax_row[1].set_title(f\"Refined - {filename}\")\n",
    "        ax_row[1].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Load and compare images\n",
    "image_folder = \"generated_images\"\n",
    "refined_folder = \"refined_images\"\n",
    "image_pairs = load_images(image_folder, refined_folder, num_images=5)\n",
    "plot_comparison(image_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# Load CLIP model for evaluation\n",
    "def load_clip_model():\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    return model, processor\n",
    "\n",
    "def compute_clip_similarity(image_path, prompt, model, processor):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(text=[prompt], images=image, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.logits_per_image.item()\n",
    "\n",
    "# Load models\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, processor = load_clip_model()\n",
    "model.to(torch_device)\n",
    "\n",
    "# Load dataset\n",
    "dataset_path = \"updated_physics_problems_dataset_2.json\"\n",
    "image_folder = \"generated_images\"\n",
    "refined_folder = \"refined_images\"\n",
    "\n",
    "df = pd.read_json(dataset_path)\n",
    "\n",
    "# Evaluate images\n",
    "evaluation_results = []\n",
    "for problem in df.itertuples():\n",
    "    problem_id = problem.Problem_ID\n",
    "    prompt = problem._asdict().get(\"3D_Diagram_Description\", \"\")\n",
    "    orig_image_path = os.path.join(image_folder, f\"problem_{problem_id}.png\")\n",
    "    refined_image_path = os.path.join(refined_folder, f\"problem_{problem_id}.png\")\n",
    "    \n",
    "    if os.path.exists(orig_image_path) and os.path.exists(refined_image_path):\n",
    "        orig_score = compute_clip_similarity(orig_image_path, prompt, model, processor)\n",
    "        refined_score = compute_clip_similarity(refined_image_path, prompt, model, processor)\n",
    "        improvement = refined_score - orig_score\n",
    "        evaluation_results.append({\n",
    "            \"Problem_ID\": problem_id,\n",
    "            \"Original_Score\": orig_score,\n",
    "            \"Refined_Score\": refined_score,\n",
    "            \"Improvement\": improvement\n",
    "        })\n",
    "\n",
    "# Save results\n",
    "evaluation_df = pd.DataFrame(evaluation_results)\n",
    "evaluation_df.to_csv(\"refinement_quality_scores.csv\", index=False)\n",
    "print(\"Refinement quality evaluation completed. Results saved to refinement_quality_scores.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load refinement quality scores\n",
    "df = pd.read_csv(\"refinement_quality_scores.csv\")\n",
    "\n",
    "# Plot distribution of improvement scores\n",
    "def plot_improvement_distribution(df):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(df[\"Improvement\"], bins=20, kde=True, color='blue')\n",
    "    plt.xlabel(\"Improvement in CLIP Similarity Score\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Distribution of Refinement Improvements\")\n",
    "    plt.show()\n",
    "\n",
    "# Scatter plot of Problem IDs vs Improvement\n",
    "def plot_improvement_scatter(df):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.scatterplot(x=df[\"Problem_ID\"], y=df[\"Improvement\"], hue=df[\"Improvement\"], palette=\"coolwarm\", size=df[\"Improvement\"], sizes=(20, 200))\n",
    "    plt.xlabel(\"Problem ID\")\n",
    "    plt.ylabel(\"Improvement in CLIP Score\")\n",
    "    plt.title(\"Improvement in Refinement Across Problems\")\n",
    "    plt.legend(title=\"Improvement Score\", bbox_to_anchor=(1, 1))\n",
    "    plt.show()\n",
    "\n",
    "# Run plots\n",
    "plot_improvement_distribution(df)\n",
    "plot_improvement_scatter(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load generated solutions\n",
    "df = json.load(open(\"flan_t5_generated_solutions.json\", \"r\"))\n",
    "\n",
    "def check_algebraic_correctness(expression, expected_result):\n",
    "    try:\n",
    "        expr = sp.sympify(expression)\n",
    "        expected = sp.sympify(expected_result)\n",
    "        return sp.simplify(expr - expected) == 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking correctness: {e}\")\n",
    "        return False\n",
    "\n",
    "def generate_problem_graph(problem_statement, solution_expression):\n",
    "    x = sp.symbols('x')\n",
    "    try:\n",
    "        expr = sp.sympify(solution_expression)\n",
    "        f_lambdified = sp.lambdify(x, expr, 'numpy')\n",
    "        \n",
    "        x_vals = np.linspace(-10, 10, 400)\n",
    "        y_vals = f_lambdified(x_vals)\n",
    "        \n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.plot(x_vals, y_vals, label=f\"y = {solution_expression}\", color='blue')\n",
    "        plt.axhline(0, color='black', linewidth=0.5)\n",
    "        plt.axvline(0, color='black', linewidth=0.5)\n",
    "        plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "        plt.title(f\"Graph for: {problem_statement}\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating graph: {e}\")\n",
    "\n",
    "# Iterate over generated solutions to check algebraic correctness\n",
    "for problem in df:\n",
    "    problem_id = problem[\"Problem_ID\"]\n",
    "    problem_statement = problem[\"Problem_Statement\"]\n",
    "    generated_solution = problem[\"Generated_Solution\"]\n",
    "    expected_answer = problem.get(\"Final_Answer\", \"\")\n",
    "    \n",
    "    if check_algebraic_correctness(generated_solution, expected_answer):\n",
    "        print(f\"Problem {problem_id}: Correct Algebraic Solution\")\n",
    "    else:\n",
    "        print(f\"Problem {problem_id}: Incorrect Algebraic Solution\")\n",
    "    \n",
    "    generate_problem_graph(problem_statement, generated_solution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# Load dataset\n",
    "with open(\"updated_physics_problems_dataset_2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "def setup_pybullet():\n",
    "    p.connect(p.DIRECT)\n",
    "    p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "    p.setGravity(0, 0, -9.8)\n",
    "\n",
    "def generate_simulation(problem_statement):\n",
    "    setup_pybullet()\n",
    "    plane_id = p.loadURDF(\"plane.urdf\")\n",
    "    box_id = p.loadURDF(\"r2d2.urdf\", [0, 0, 1])\n",
    "    \n",
    "    video_frames = []\n",
    "    for i in range(100):\n",
    "        p.stepSimulation()\n",
    "        frame = p.getCameraImage(320, 240)[2]\n",
    "        frame = np.array(frame, dtype=np.uint8)\n",
    "        video_frames.append(frame)\n",
    "    \n",
    "    p.disconnect()\n",
    "    return video_frames\n",
    "\n",
    "def save_simulation_video(frames, output_path=\"simulation_output.avi\"):\n",
    "    height, width, _ = frames[0].shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, 20.0, (width, height))\n",
    "    \n",
    "    for frame in frames:\n",
    "        out.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "    out.release()\n",
    "    print(f\"Simulation video saved at {output_path}\")\n",
    "\n",
    "def calculate_ssim(original_image, generated_image):\n",
    "    original_gray = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)\n",
    "    generated_gray = cv2.cvtColor(generated_image, cv2.COLOR_BGR2GRAY)\n",
    "    return ssim(original_gray, generated_gray)\n",
    "\n",
    "# Process dataset\n",
    "for problem in dataset[:5]:  # Limiting to first 5 for efficiency\n",
    "    problem_id = problem[\"Problem_ID\"]\n",
    "    problem_statement = problem[\"Problem_Statement\"]\n",
    "    \n",
    "    print(f\"Generating simulation for Problem {problem_id}...\")\n",
    "    frames = generate_simulation(problem_statement)\n",
    "    save_simulation_video(frames, f\"simulation_videos/problem_{problem_id}.avi\")\n",
    "    \n",
    "    # Validate last frame against first frame using SSIM\n",
    "    similarity_score = calculate_ssim(frames[0], frames[-1])\n",
    "    print(f\"Problem {problem_id} - SSIM Score: {similarity_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, T5Tokenizer, T5ForConditionalGeneration, LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "# Load models\n",
    "def load_bert_model(model_path=\"bert-base-uncased\"):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path, num_labels=3)\n",
    "    return tokenizer, model\n",
    "\n",
    "def load_flan_t5_model(model_path=\"google/flan-t5-large\"):\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "    return tokenizer, model\n",
    "\n",
    "def load_llama_model(model_path=\"meta-llama/Llama-2-7b-chat-hf\"):\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "    model = LlamaForCausalLM.from_pretrained(model_path)\n",
    "    return tokenizer, model\n",
    "\n",
    "def classify_problem(problem_statement, tokenizer, model):\n",
    "    inputs = tokenizer(problem_statement, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "    label_map = {0: \"Easy\", 1: \"Medium\", 2: \"Hard\"}\n",
    "    return label_map[prediction]\n",
    "\n",
    "def generate_explanation(problem_statement, tokenizer, model, max_length=200):\n",
    "    input_text = f\"Explain: {problem_statement}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_length=max_length)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "def setup_pybullet():\n",
    "    p.connect(p.DIRECT)\n",
    "    p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "    p.setGravity(0, 0, -9.8)\n",
    "\n",
    "def generate_simulation(simulation_description):\n",
    "    setup_pybullet()\n",
    "    plane_id = p.loadURDF(\"plane.urdf\")\n",
    "    box_id = p.loadURDF(\"r2d2.urdf\", [0, 0, 1])\n",
    "    \n",
    "    video_frames = []\n",
    "    for _ in range(100):\n",
    "        p.stepSimulation()\n",
    "        frame = p.getCameraImage(320, 240)[2]\n",
    "        frame = np.array(frame, dtype=np.uint8)\n",
    "        video_frames.append(frame)\n",
    "    \n",
    "    p.disconnect()\n",
    "    return video_frames\n",
    "\n",
    "def save_simulation_video(frames, output_path):\n",
    "    height, width, _ = frames[0].shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, 20.0, (width, height))\n",
    "    \n",
    "    for frame in frames:\n",
    "        out.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "    out.release()\n",
    "    print(f\"Simulation video saved at {output_path}\")\n",
    "\n",
    "# Load models\n",
    "tokenizer_bert, model_bert = load_bert_model()\n",
    "tokenizer_flan, model_flan = load_flan_t5_model()\n",
    "tokenizer_llama, model_llama = load_llama_model()\n",
    "\n",
    "# Load dataset\n",
    "with open(\"updated_physics_problems_dataset_2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# Process dataset\n",
    "for problem in dataset[:5]:  # Limiting to first 5 for efficiency\n",
    "    problem_id = problem[\"Problem_ID\"]\n",
    "    problem_statement = problem[\"Problem_Statement\"]\n",
    "    simulation_description = problem.get(\"Simulation_Description\", \"\")\n",
    "    \n",
    "    difficulty = classify_problem(problem_statement, tokenizer_bert, model_bert)\n",
    "    explanation = generate_explanation(problem_statement, tokenizer_flan, model_flan)\n",
    "    print(f\"Problem {problem_id} - Difficulty: {difficulty}\\nExplanation: {explanation}\")\n",
    "    \n",
    "    print(f\"Generating simulation for Problem {problem_id}...\")\n",
    "    frames = generate_simulation(simulation_description)\n",
    "    save_simulation_video(frames, f\"simulation_videos/problem_{problem_id}.avi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, T5Tokenizer, T5ForConditionalGeneration, LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "# Load models\n",
    "def load_bert_model(model_path=\"bert-base-uncased\"):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path, num_labels=3)\n",
    "    return tokenizer, model\n",
    "\n",
    "def load_flan_t5_model(model_path=\"google/flan-t5-large\"):\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "    return tokenizer, model\n",
    "\n",
    "def load_llama_model(model_path=\"meta-llama/Llama-2-7b-chat-hf\"):\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "    model = LlamaForCausalLM.from_pretrained(model_path)\n",
    "    return tokenizer, model\n",
    "\n",
    "def classify_problem(problem_statement, tokenizer, model):\n",
    "    inputs = tokenizer(problem_statement, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "    label_map = {0: \"Easy\", 1: \"Medium\", 2: \"Hard\"}\n",
    "    return label_map[prediction]\n",
    "\n",
    "def generate_explanation(problem_statement, tokenizer, model, max_length=200):\n",
    "    input_text = f\"Explain: {problem_statement}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_length=max_length)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "def setup_pybullet():\n",
    "    p.connect(p.DIRECT)\n",
    "    p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "    p.setGravity(0, 0, -9.8)\n",
    "\n",
    "def generate_simulation(simulation_description):\n",
    "    setup_pybullet()\n",
    "    plane_id = p.loadURDF(\"plane.urdf\")\n",
    "    box_id = p.loadURDF(\"r2d2.urdf\", [0, 0, 1])\n",
    "    \n",
    "    video_frames = []\n",
    "    for _ in range(100):\n",
    "        p.stepSimulation()\n",
    "        frame = p.getCameraImage(320, 240)[2]\n",
    "        frame = np.array(frame, dtype=np.uint8)\n",
    "        video_frames.append(frame)\n",
    "    \n",
    "    p.disconnect()\n",
    "    return video_frames\n",
    "\n",
    "def save_simulation_video(frames, output_path):\n",
    "    height, width, _ = frames[0].shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, 20.0, (width, height))\n",
    "    \n",
    "    for frame in frames:\n",
    "        out.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "    out.release()\n",
    "    print(f\"Simulation video saved at {output_path}\")\n",
    "\n",
    "def calculate_ssim(original_frame, generated_frame):\n",
    "    original_gray = cv2.cvtColor(original_frame, cv2.COLOR_BGR2GRAY)\n",
    "    generated_gray = cv2.cvtColor(generated_frame, cv2.COLOR_BGR2GRAY)\n",
    "    return ssim(original_gray, generated_gray)\n",
    "\n",
    "# Load models\n",
    "tokenizer_bert, model_bert = load_bert_model()\n",
    "tokenizer_flan, model_flan = load_flan_t5_model()\n",
    "tokenizer_llama, model_llama = load_llama_model()\n",
    "\n",
    "# Load dataset\n",
    "with open(\"updated_physics_problems_dataset_2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# Process dataset\n",
    "for problem in dataset[:5]:  # Limiting to first 5 for efficiency\n",
    "    problem_id = problem[\"Problem_ID\"]\n",
    "    problem_statement = problem[\"Problem_Statement\"]\n",
    "    simulation_description = problem.get(\"Simulation_Description\", \"\")\n",
    "    \n",
    "    difficulty = classify_problem(problem_statement, tokenizer_bert, model_bert)\n",
    "    explanation = generate_explanation(problem_statement, tokenizer_flan, model_flan)\n",
    "    print(f\"Problem {problem_id} - Difficulty: {difficulty}\\nExplanation: {explanation}\")\n",
    "    \n",
    "    print(f\"Generating simulation for Problem {problem_id}...\")\n",
    "    frames = generate_simulation(simulation_description)\n",
    "    save_simulation_video(frames, f\"simulation_videos/problem_{problem_id}.avi\")\n",
    "    \n",
    "    # Validate simulation using SSIM\n",
    "    similarity_score = calculate_ssim(frames[0], frames[-1])\n",
    "    print(f\"Problem {problem_id} - SSIM Score: {similarity_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, T5Tokenizer, T5ForConditionalGeneration, LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "# Load models\n",
    "def load_bert_model(model_path=\"bert-base-uncased\"):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path, num_labels=3)\n",
    "    return tokenizer, model\n",
    "\n",
    "def load_flan_t5_model(model_path=\"google/flan-t5-large\"):\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "    return tokenizer, model\n",
    "\n",
    "def load_llama_model(model_path=\"meta-llama/Llama-2-7b-chat-hf\"):\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "    model = LlamaForCausalLM.from_pretrained(model_path)\n",
    "    return tokenizer, model\n",
    "\n",
    "def classify_problem(problem_statement, tokenizer, model):\n",
    "    inputs = tokenizer(problem_statement, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "    label_map = {0: \"Easy\", 1: \"Medium\", 2: \"Hard\"}\n",
    "    return label_map[prediction]\n",
    "\n",
    "def generate_explanation(problem_statement, tokenizer, model, max_length=200):\n",
    "    input_text = f\"Explain: {problem_statement}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_length=max_length)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "def setup_pybullet():\n",
    "    p.connect(p.DIRECT)\n",
    "    p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "    p.setGravity(0, 0, -9.8)\n",
    "\n",
    "def generate_simulation(simulation_description):\n",
    "    setup_pybullet()\n",
    "    plane_id = p.loadURDF(\"plane.urdf\")\n",
    "    box_id = p.loadURDF(\"r2d2.urdf\", [0, 0, 1])\n",
    "    \n",
    "    video_frames = []\n",
    "    for _ in range(100):\n",
    "        p.stepSimulation()\n",
    "        frame = p.getCameraImage(320, 240)[2]\n",
    "        frame = np.array(frame, dtype=np.uint8)\n",
    "        video_frames.append(frame)\n",
    "    \n",
    "    p.disconnect()\n",
    "    return video_frames\n",
    "\n",
    "def save_simulation_video(frames, output_path):\n",
    "    height, width, _ = frames[0].shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, 20.0, (width, height))\n",
    "    \n",
    "    for frame in frames:\n",
    "        out.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "    out.release()\n",
    "    print(f\"Simulation video saved at {output_path}\")\n",
    "\n",
    "def calculate_ssim(original_frame, generated_frame):\n",
    "    original_gray = cv2.cvtColor(original_frame, cv2.COLOR_BGR2GRAY)\n",
    "    generated_gray = cv2.cvtColor(generated_frame, cv2.COLOR_BGR2GRAY)\n",
    "    return ssim(original_gray, generated_gray)\n",
    "\n",
    "def plot_ssim_scores(ssim_scores):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(ssim_scores, bins=10, kde=True, color='blue')\n",
    "    plt.xlabel(\"SSIM Score\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Distribution of SSIM Scores Across Simulations\")\n",
    "    plt.show()\n",
    "\n",
    "# Load models\n",
    "tokenizer_bert, model_bert = load_bert_model()\n",
    "tokenizer_flan, model_flan = load_flan_t5_model()\n",
    "tokenizer_llama, model_llama = load_llama_model()\n",
    "\n",
    "# Load dataset\n",
    "with open(\"updated_physics_problems_dataset_2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "ssim_scores = []\n",
    "# Process dataset\n",
    "for problem in dataset[:5]:  # Limiting to first 5 for efficiency\n",
    "    problem_id = problem[\"Problem_ID\"]\n",
    "    problem_statement = problem[\"Problem_Statement\"]\n",
    "    simulation_description = problem.get(\"Simulation_Description\", \"\")\n",
    "    \n",
    "    difficulty = classify_problem(problem_statement, tokenizer_bert, model_bert)\n",
    "    explanation = generate_explanation(problem_statement, tokenizer_flan, model_flan)\n",
    "    print(f\"Problem {problem_id} - Difficulty: {difficulty}\\nExplanation: {explanation}\")\n",
    "    \n",
    "    print(f\"Generating simulation for Problem {problem_id}...\")\n",
    "    frames = generate_simulation(simulation_description)\n",
    "    save_simulation_video(frames, f\"simulation_videos/problem_{problem_id}.avi\")\n",
    "    \n",
    "    # Validate simulation using SSIM\n",
    "    similarity_score = calculate_ssim(frames[0], frames[-1])\n",
    "    ssim_scores.append(similarity_score)\n",
    "    print(f\"Problem {problem_id} - SSIM Score: {similarity_score:.4f}\")\n",
    "\n",
    "# Plot SSIM scores\n",
    "total_problems = len(ssim_scores)\n",
    "if total_problems > 0:\n",
    "    plot_ssim_scores(ssim_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, T5Tokenizer, T5ForConditionalGeneration, LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "# Load models\n",
    "def load_bert_model(model_path=\"bert-base-uncased\"):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path, num_labels=3)\n",
    "    return tokenizer, model\n",
    "\n",
    "def load_flan_t5_model(model_path=\"google/flan-t5-large\"):\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "    return tokenizer, model\n",
    "\n",
    "def load_llama_model(model_path=\"meta-llama/Llama-2-7b-chat-hf\"):\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "    model = LlamaForCausalLM.from_pretrained(model_path)\n",
    "    return tokenizer, model\n",
    "\n",
    "def classify_problem(problem_statement, tokenizer, model):\n",
    "    inputs = tokenizer(problem_statement, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "    label_map = {0: \"Easy\", 1: \"Medium\", 2: \"Hard\"}\n",
    "    return label_map[prediction]\n",
    "\n",
    "def generate_explanation(problem_statement, tokenizer, model, max_length=200):\n",
    "    input_text = f\"Explain: {problem_statement}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_length=max_length)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "def setup_pybullet():\n",
    "    p.connect(p.DIRECT)\n",
    "    p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "    p.setGravity(0, 0, -9.8)\n",
    "\n",
    "def generate_simulation(simulation_description):\n",
    "    setup_pybullet()\n",
    "    plane_id = p.loadURDF(\"plane.urdf\")\n",
    "    box_id = p.loadURDF(\"r2d2.urdf\", [0, 0, 1])\n",
    "    \n",
    "    video_frames = []\n",
    "    for _ in range(100):\n",
    "        p.stepSimulation()\n",
    "        frame = p.getCameraImage(320, 240)[2]\n",
    "        frame = np.array(frame, dtype=np.uint8)\n",
    "        video_frames.append(frame)\n",
    "    \n",
    "    p.disconnect()\n",
    "    return video_frames\n",
    "\n",
    "def save_simulation_video(frames, output_path):\n",
    "    height, width, _ = frames[0].shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, 20.0, (width, height))\n",
    "    \n",
    "    for frame in frames:\n",
    "        out.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "    out.release()\n",
    "    print(f\"Simulation video saved at {output_path}\")\n",
    "\n",
    "def calculate_ssim(original_frame, generated_frame):\n",
    "    original_gray = cv2.cvtColor(original_frame, cv2.COLOR_BGR2GRAY)\n",
    "    generated_gray = cv2.cvtColor(generated_frame, cv2.COLOR_BGR2GRAY)\n",
    "    return ssim(original_gray, generated_gray)\n",
    "\n",
    "def plot_ssim_scores(ssim_scores):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(ssim_scores, bins=10, kde=True, color='blue')\n",
    "    plt.xlabel(\"SSIM Score\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Distribution of SSIM Scores Across Simulations\")\n",
    "    plt.show()\n",
    "\n",
    "def generate_ssim_report(ssim_scores, output_file=\"ssim_report.txt\"):\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(\"SSIM Score Report\\n\")\n",
    "        f.write(\"===================\\n\\n\")\n",
    "        f.write(f\"Total Simulations Evaluated: {len(ssim_scores)}\\n\")\n",
    "        f.write(f\"Average SSIM Score: {np.mean(ssim_scores):.4f}\\n\")\n",
    "        f.write(f\"Max SSIM Score: {np.max(ssim_scores):.4f}\\n\")\n",
    "        f.write(f\"Min SSIM Score: {np.min(ssim_scores):.4f}\\n\")\n",
    "    print(f\"SSIM report saved at {output_file}\")\n",
    "\n",
    "# Load models\n",
    "tokenizer_bert, model_bert = load_bert_model()\n",
    "tokenizer_flan, model_flan = load_flan_t5_model()\n",
    "tokenizer_llama, model_llama = load_llama_model()\n",
    "\n",
    "# Load dataset\n",
    "with open(\"updated_physics_problems_dataset_2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "ssim_scores = []\n",
    "# Process dataset\n",
    "for problem in dataset[:5]:  # Limiting to first 5 for efficiency\n",
    "    problem_id = problem[\"Problem_ID\"]\n",
    "    problem_statement = problem[\"Problem_Statement\"]\n",
    "    simulation_description = problem.get(\"Simulation_Description\", \"\")\n",
    "    \n",
    "    difficulty = classify_problem(problem_statement, tokenizer_bert, model_bert)\n",
    "    explanation = generate_explanation(problem_statement, tokenizer_flan, model_flan)\n",
    "    print(f\"Problem {problem_id} - Difficulty: {difficulty}\\nExplanation: {explanation}\")\n",
    "    \n",
    "    print(f\"Generating simulation for Problem {problem_id}...\")\n",
    "    frames = generate_simulation(simulation_description)\n",
    "    save_simulation_video(frames, f\"simulation_videos/problem_{problem_id}.avi\")\n",
    "    \n",
    "    # Validate simulation using SSIM\n",
    "    similarity_score = calculate_ssim(frames[0], frames[-1])\n",
    "    ssim_scores.append(similarity_score)\n",
    "    print(f\"Problem {problem_id} - SSIM Score: {similarity_score:.4f}\")\n",
    "\n",
    "# Plot SSIM scores\n",
    "if ssim_scores:\n",
    "    plot_ssim_scores(ssim_scores)\n",
    "    generate_ssim_report(ssim_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from manim import *\n",
    "import bpy  # Blender Python API\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, T5Tokenizer, T5ForConditionalGeneration, LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "# Load models\n",
    "def load_bert_model(model_path=\"bert-base-uncased\"):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path, num_labels=3)\n",
    "    return tokenizer, model\n",
    "\n",
    "def load_flan_t5_model(model_path=\"google/flan-t5-large\"):\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "    return tokenizer, model\n",
    "\n",
    "def load_llama_model(model_path=\"meta-llama/Llama-2-7b-chat-hf\"):\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "    model = LlamaForCausalLM.from_pretrained(model_path)\n",
    "    return tokenizer, model\n",
    "\n",
    "def setup_blender_simulation():\n",
    "    bpy.ops.wm.read_factory_settings(use_empty=True)\n",
    "    bpy.ops.mesh.primitive_cube_add(size=2, location=(0, 0, 1))\n",
    "    bpy.ops.rigidbody.object_add()\n",
    "    bpy.ops.mesh.primitive_plane_add(size=10, location=(0, 0, 0))\n",
    "    bpy.ops.rigidbody.object_add()\n",
    "    bpy.context.object.rigid_body.type = 'PASSIVE'\n",
    "\n",
    "def run_blender_simulation(output_path=\"blender_simulation.mp4\"):\n",
    "    bpy.context.scene.frame_start = 1\n",
    "    bpy.context.scene.frame_end = 100\n",
    "    bpy.context.scene.render.filepath = output_path\n",
    "    bpy.ops.render.render(animation=True)\n",
    "    print(f\"Blender physics simulation saved at {output_path}\")\n",
    "\n",
    "def classify_problem(problem_statement, tokenizer, model):\n",
    "    inputs = tokenizer(problem_statement, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "    label_map = {0: \"Easy\", 1: \"Medium\", 2: \"Hard\"}\n",
    "    return label_map[prediction]\n",
    "\n",
    "def generate_explanation(problem_statement, tokenizer, model, max_length=200):\n",
    "    input_text = f\"Explain: {problem_statement}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_length=max_length)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "def calculate_ssim(original_frame, generated_frame):\n",
    "    original_gray = cv2.cvtColor(original_frame, cv2.COLOR_BGR2GRAY)\n",
    "    generated_gray = cv2.cvtColor(generated_frame, cv2.COLOR_BGR2GRAY)\n",
    "    return ssim(original_gray, generated_gray)\n",
    "\n",
    "def plot_ssim_scores(ssim_scores):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(ssim_scores, bins=10, kde=True, color='blue')\n",
    "    plt.xlabel(\"SSIM Score\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Distribution of SSIM Scores Across Simulations\")\n",
    "    plt.show()\n",
    "\n",
    "def generate_ssim_report(ssim_scores, output_file=\"ssim_report.txt\"):\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(\"SSIM Score Report\\n\")\n",
    "        f.write(\"===================\\n\\n\")\n",
    "        f.write(f\"Total Simulations Evaluated: {len(ssim_scores)}\\n\")\n",
    "        f.write(f\"Average SSIM Score: {np.mean(ssim_scores):.4f}\\n\")\n",
    "        f.write(f\"Max SSIM Score: {np.max(ssim_scores):.4f}\\n\")\n",
    "        f.write(f\"Min SSIM Score: {np.min(ssim_scores):.4f}\\n\")\n",
    "    print(f\"SSIM report saved at {output_file}\")\n",
    "\n",
    "class MathVisualization(Scene):\n",
    "    def construct(self):\n",
    "        equation = MathTex(\"F = ma\")\n",
    "        self.play(Write(equation))\n",
    "        self.wait(2)\n",
    "\n",
    "# Load models\n",
    "tokenizer_bert, model_bert = load_bert_model()\n",
    "tokenizer_flan, model_flan = load_flan_t5_model()\n",
    "tokenizer_llama, model_llama = load_llama_model()\n",
    "\n",
    "# Load dataset\n",
    "with open(\"updated_physics_problems_dataset_2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "ssim_scores = []\n",
    "# Process dataset\n",
    "for problem in dataset[:5]:  # Limiting to first 5 for efficiency\n",
    "    problem_id = problem[\"Problem_ID\"]\n",
    "    problem_statement = problem[\"Problem_Statement\"]\n",
    "    \n",
    "    difficulty = classify_problem(problem_statement, tokenizer_bert, model_bert)\n",
    "    explanation = generate_explanation(problem_statement, tokenizer_flan, model_flan)\n",
    "    print(f\"Problem {problem_id} - Difficulty: {difficulty}\\nExplanation: {explanation}\")\n",
    "    \n",
    "    print(f\"Running Blender physics simulation for Problem {problem_id}...\")\n",
    "    setup_blender_simulation()\n",
    "    run_blender_simulation(f\"blender_simulations/problem_{problem_id}.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from manim import *\n",
    "import bpy  # Blender Python API\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, T5Tokenizer, T5ForConditionalGeneration, LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "# Load models\n",
    "def load_bert_model(model_path=\"bert-base-uncased\"):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path, num_labels=3)\n",
    "    return tokenizer, model\n",
    "\n",
    "def load_flan_t5_model(model_path=\"google/flan-t5-large\"):\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "    return tokenizer, model\n",
    "\n",
    "def load_llama_model(model_path=\"meta-llama/Llama-2-7b-chat-hf\"):\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "    model = LlamaForCausalLM.from_pretrained(model_path)\n",
    "    return tokenizer, model\n",
    "\n",
    "def setup_blender_simulation():\n",
    "    bpy.ops.wm.read_factory_settings(use_empty=True)\n",
    "    bpy.ops.mesh.primitive_cube_add(size=2, location=(0, 0, 1))\n",
    "    bpy.ops.rigidbody.object_add()\n",
    "    bpy.ops.mesh.primitive_plane_add(size=10, location=(0, 0, 0))\n",
    "    bpy.ops.rigidbody.object_add()\n",
    "    bpy.context.object.rigid_body.type = 'PASSIVE'\n",
    "\n",
    "def run_blender_simulation(output_path=\"blender_simulation.mp4\"):\n",
    "    bpy.context.scene.frame_start = 1\n",
    "    bpy.context.scene.frame_end = 100\n",
    "    bpy.context.scene.render.filepath = output_path\n",
    "    bpy.ops.render.render(animation=True)\n",
    "    print(f\"Blender physics simulation saved at {output_path}\")\n",
    "\n",
    "def classify_problem(problem_statement, tokenizer, model):\n",
    "    inputs = tokenizer(problem_statement, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "    label_map = {0: \"Easy\", 1: \"Medium\", 2: \"Hard\"}\n",
    "    return label_map[prediction]\n",
    "\n",
    "def generate_explanation(problem_statement, tokenizer, model, max_length=200):\n",
    "    input_text = f\"Explain: {problem_statement}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_length=max_length)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "def compare_pybullet_blender(pybullet_video, blender_video):\n",
    "    print(f\"Comparing PyBullet Simulation: {pybullet_video} with Blender Simulation: {blender_video}\")\n",
    "    pyb_vid = cv2.VideoCapture(pybullet_video)\n",
    "    blend_vid = cv2.VideoCapture(blender_video)\n",
    "    ssim_scores = []\n",
    "    \n",
    "    while True:\n",
    "        ret1, frame1 = pyb_vid.read()\n",
    "        ret2, frame2 = blend_vid.read()\n",
    "        \n",
    "        if not ret1 or not ret2:\n",
    "            break\n",
    "        \n",
    "        frame1_gray = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "        frame2_gray = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        score = ssim(frame1_gray, frame2_gray)\n",
    "        ssim_scores.append(score)\n",
    "    \n",
    "    pyb_vid.release()\n",
    "    blend_vid.release()\n",
    "    \n",
    "    avg_ssim = np.mean(ssim_scores)\n",
    "    print(f\"Average SSIM Score between PyBullet and Blender: {avg_ssim:.4f}\")\n",
    "    return avg_ssim\n",
    "\n",
    "def generate_ssim_report(ssim_scores, output_file=\"ssim_report.txt\"):\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(\"SSIM Score Report\\n\")\n",
    "        f.write(\"===================\\n\\n\")\n",
    "        f.write(f\"Total Simulations Evaluated: {len(ssim_scores)}\\n\")\n",
    "        f.write(f\"Average SSIM Score: {np.mean(ssim_scores):.4f}\\n\")\n",
    "        f.write(f\"Max SSIM Score: {np.max(ssim_scores):.4f}\\n\")\n",
    "        f.write(f\"Min SSIM Score: {np.min(ssim_scores):.4f}\\n\")\n",
    "    print(f\"SSIM report saved at {output_file}\")\n",
    "\n",
    "class MathVisualization(Scene):\n",
    "    def construct(self):\n",
    "        equation = MathTex(\"F = ma\")\n",
    "        self.play(Write(equation))\n",
    "        self.wait(2)\n",
    "\n",
    "# Load models\n",
    "tokenizer_bert, model_bert = load_bert_model()\n",
    "tokenizer_flan, model_flan = load_flan_t5_model()\n",
    "tokenizer_llama, model_llama = load_llama_model()\n",
    "\n",
    "# Load dataset\n",
    "with open(\"updated_physics_problems_dataset_2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "ssim_scores = []\n",
    "# Process dataset\n",
    "for problem in dataset[:5]:  # Limiting to first 5 for efficiency\n",
    "    problem_id = problem[\"Problem_ID\"]\n",
    "    problem_statement = problem[\"Problem_Statement\"]\n",
    "    \n",
    "    difficulty = classify_problem(problem_statement, tokenizer_bert, model_bert)\n",
    "    explanation = generate_explanation(problem_statement, tokenizer_flan, model_flan)\n",
    "    print(f\"Problem {problem_id} - Difficulty: {difficulty}\\nExplanation: {explanation}\")\n",
    "    \n",
    "    print(f\"Running Blender physics simulation for Problem {problem_id}...\")\n",
    "    setup_blender_simulation()\n",
    "    blender_video = f\"blender_simulations/problem_{problem_id}.mp4\"\n",
    "    run_blender_simulation(blender_video)\n",
    "    \n",
    "    pybullet_video = f\"pybullet_simulations/problem_{problem_id}.mp4\"  # Placeholder for future PyBullet video\n",
    "    avg_ssim = compare_pybullet_blender(pybullet_video, blender_video)\n",
    "    ssim_scores.append(avg_ssim)\n",
    "    \n",
    "if ssim_scores:\n",
    "    generate_ssim_report(ssim_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from manim import *\n",
    "import bpy  # Blender Python API\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, T5Tokenizer, T5ForConditionalGeneration, LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "# Load models\n",
    "def load_bert_model(model_path=\"bert-base-uncased\"):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path, num_labels=3)\n",
    "    return tokenizer, model\n",
    "\n",
    "def load_flan_t5_model(model_path=\"google/flan-t5-large\"):\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "    return tokenizer, model\n",
    "\n",
    "def load_llama_model(model_path=\"meta-llama/Llama-2-7b-chat-hf\"):\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "    model = LlamaForCausalLM.from_pretrained(model_path)\n",
    "    return tokenizer, model\n",
    "\n",
    "def setup_blender_simulation():\n",
    "    bpy.ops.wm.read_factory_settings(use_empty=True)\n",
    "    bpy.ops.mesh.primitive_cube_add(size=2, location=(0, 0, 1))\n",
    "    bpy.ops.rigidbody.object_add()\n",
    "    bpy.ops.mesh.primitive_plane_add(size=10, location=(0, 0, 0))\n",
    "    bpy.ops.rigidbody.object_add()\n",
    "    bpy.context.object.rigid_body.type = 'PASSIVE'\n",
    "\n",
    "def run_blender_simulation(output_path=\"blender_simulation.mp4\"):\n",
    "    bpy.context.scene.frame_start = 1\n",
    "    bpy.context.scene.frame_end = 100\n",
    "    bpy.context.scene.render.filepath = output_path\n",
    "    bpy.ops.render.render(animation=True)\n",
    "    print(f\"Blender physics simulation saved at {output_path}\")\n",
    "\n",
    "def classify_problem(problem_statement, tokenizer, model):\n",
    "    inputs = tokenizer(problem_statement, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "    label_map = {0: \"Easy\", 1: \"Medium\", 2: \"Hard\"}\n",
    "    return label_map[prediction]\n",
    "\n",
    "def generate_explanation(problem_statement, tokenizer, model, max_length=200):\n",
    "    input_text = f\"Explain: {problem_statement}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_length=max_length)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "def compare_pybullet_blender(pybullet_video, blender_video):\n",
    "    print(f\"Comparing PyBullet Simulation: {pybullet_video} with Blender Simulation: {blender_video}\")\n",
    "    pyb_vid = cv2.VideoCapture(pybullet_video)\n",
    "    blend_vid = cv2.VideoCapture(blender_video)\n",
    "    ssim_scores = []\n",
    "    \n",
    "    while True:\n",
    "        ret1, frame1 = pyb_vid.read()\n",
    "        ret2, frame2 = blend_vid.read()\n",
    "        \n",
    "        if not ret1 or not ret2:\n",
    "            break\n",
    "        \n",
    "        frame1_gray = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "        frame2_gray = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        score = ssim(frame1_gray, frame2_gray)\n",
    "        ssim_scores.append(score)\n",
    "    \n",
    "    pyb_vid.release()\n",
    "    blend_vid.release()\n",
    "    \n",
    "    avg_ssim = np.mean(ssim_scores)\n",
    "    print(f\"Average SSIM Score between PyBullet and Blender: {avg_ssim:.4f}\")\n",
    "    return avg_ssim\n",
    "\n",
    "def plot_ssim_comparison(ssim_scores):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(ssim_scores, bins=10, kde=True, color='blue')\n",
    "    plt.xlabel(\"SSIM Score\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"SSIM Comparison: PyBullet vs Blender Simulations\")\n",
    "    plt.show()\n",
    "\n",
    "def generate_ssim_report(ssim_scores, output_file=\"ssim_report.txt\"):\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(\"SSIM Score Report\\n\")\n",
    "        f.write(\"===================\\n\\n\")\n",
    "        f.write(f\"Total Simulations Evaluated: {len(ssim_scores)}\\n\")\n",
    "        f.write(f\"Average SSIM Score: {np.mean(ssim_scores):.4f}\\n\")\n",
    "        f.write(f\"Max SSIM Score: {np.max(ssim_scores):.4f}\\n\")\n",
    "        f.write(f\"Min SSIM Score: {np.min(ssim_scores):.4f}\\n\")\n",
    "    print(f\"SSIM report saved at {output_file}\")\n",
    "\n",
    "class MathVisualization(Scene):\n",
    "    def construct(self):\n",
    "        equation = MathTex(\"F = ma\")\n",
    "        self.play(Write(equation))\n",
    "        self.wait(2)\n",
    "\n",
    "# Load models\n",
    "tokenizer_bert, model_bert = load_bert_model()\n",
    "tokenizer_flan, model_flan = load_flan_t5_model()\n",
    "tokenizer_llama, model_llama = load_llama_model()\n",
    "\n",
    "# Load dataset\n",
    "with open(\"updated_physics_problems_dataset_2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "ssim_scores = []\n",
    "# Process dataset\n",
    "for problem in dataset[:5]:  # Limiting to first 5 for efficiency\n",
    "    problem_id = problem[\"Problem_ID\"]\n",
    "    problem_statement = problem[\"Problem_Statement\"]\n",
    "    \n",
    "    difficulty = classify_problem(problem_statement, tokenizer_bert, model_bert)\n",
    "    explanation = generate_explanation(problem_statement, tokenizer_flan, model_flan)\n",
    "    print(f\"Problem {problem_id} - Difficulty: {difficulty}\\nExplanation: {explanation}\")\n",
    "    \n",
    "    print(f\"Running Blender physics simulation for Problem {problem_id}...\")\n",
    "    setup_blender_simulation()\n",
    "    blender_video = f\"blender_simulations/problem_{problem_id}.mp4\"\n",
    "    run_blender_simulation(blender_video)\n",
    "    \n",
    "    pybullet_video = f\"pybullet_simulations/problem_{problem_id}.mp4\"  # Placeholder for future PyBullet video\n",
    "    avg_ssim = compare_pybullet_blender(pybullet_video, blender_video)\n",
    "    ssim_scores.append(avg_ssim)\n",
    "    \n",
    "if ssim_scores:\n",
    "    plot_ssim_comparison(ssim_scores)\n",
    "    generate_ssim_report(ssim_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from manim import *\n",
    "import bpy  # Blender Python API\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, T5Tokenizer, T5ForConditionalGeneration, LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "# Load models\n",
    "def load_bert_model(model_path=\"bert-base-uncased\"):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path, num_labels=3)\n",
    "    return tokenizer, model\n",
    "\n",
    "def load_flan_t5_model(model_path=\"google/flan-t5-large\"):\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "    return tokenizer, model\n",
    "\n",
    "def load_llama_model(model_path=\"meta-llama/Llama-2-7b-chat-hf\"):\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "    model = LlamaForCausalLM.from_pretrained(model_path)\n",
    "    return tokenizer, model\n",
    "\n",
    "def compare_pybullet_blender(pybullet_video, blender_video):\n",
    "    print(f\"Comparing PyBullet Simulation: {pybullet_video} with Blender Simulation: {blender_video}\")\n",
    "    pyb_vid = cv2.VideoCapture(pybullet_video)\n",
    "    blend_vid = cv2.VideoCapture(blender_video)\n",
    "    ssim_scores = []\n",
    "    \n",
    "    while True:\n",
    "        ret1, frame1 = pyb_vid.read()\n",
    "        ret2, frame2 = blend_vid.read()\n",
    "        \n",
    "        if not ret1 or not ret2:\n",
    "            break\n",
    "        \n",
    "        frame1_gray = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "        frame2_gray = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        score = ssim(frame1_gray, frame2_gray)\n",
    "        ssim_scores.append(score)\n",
    "    \n",
    "    pyb_vid.release()\n",
    "    blend_vid.release()\n",
    "    \n",
    "    avg_ssim = np.mean(ssim_scores)\n",
    "    print(f\"Average SSIM Score between PyBullet and Blender: {avg_ssim:.4f}\")\n",
    "    return avg_ssim\n",
    "\n",
    "def rank_simulations(simulation_scores):\n",
    "    ranked_simulations = sorted(simulation_scores, key=lambda x: x[\"ssim\"], reverse=True)\n",
    "    print(\"\\nRanked Simulations (Best to Worst):\")\n",
    "    for rank, sim in enumerate(ranked_simulations, start=1):\n",
    "        print(f\"Rank {rank}: Problem {sim['problem_id']} - SSIM: {sim['ssim']:.4f}\")\n",
    "\n",
    "def generate_ssim_report(ssim_scores, output_file=\"ssim_report.txt\"):\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(\"SSIM Score Report\\n\")\n",
    "        f.write(\"===================\\n\\n\")\n",
    "        f.write(f\"Total Simulations Evaluated: {len(ssim_scores)}\\n\")\n",
    "        f.write(f\"Average SSIM Score: {np.mean(ssim_scores):.4f}\\n\")\n",
    "        f.write(f\"Max SSIM Score: {np.max(ssim_scores):.4f}\\n\")\n",
    "        f.write(f\"Min SSIM Score: {np.min(ssim_scores):.4f}\\n\")\n",
    "    print(f\"SSIM report saved at {output_file}\")\n",
    "\n",
    "# Load models\n",
    "tokenizer_bert, model_bert = load_bert_model()\n",
    "tokenizer_flan, model_flan = load_flan_t5_model()\n",
    "tokenizer_llama, model_llama = load_llama_model()\n",
    "\n",
    "# Load dataset\n",
    "with open(\"updated_physics_problems_dataset_2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "ssim_scores = []\n",
    "simulation_results = []\n",
    "# Process dataset\n",
    "for problem in dataset[:5]:  # Limiting to first 5 for efficiency\n",
    "    problem_id = problem[\"Problem_ID\"]\n",
    "    \n",
    "    print(f\"Comparing PyBullet and Blender physics simulations for Problem {problem_id}...\")\n",
    "    pybullet_video = f\"pybullet_simulations/problem_{problem_id}.mp4\"\n",
    "    blender_video = f\"blender_simulations/problem_{problem_id}.mp4\"\n",
    "    \n",
    "    avg_ssim = compare_pybullet_blender(pybullet_video, blender_video)\n",
    "    ssim_scores.append(avg_ssim)\n",
    "    simulation_results.append({\"problem_id\": problem_id, \"ssim\": avg_ssim})\n",
    "    \n",
    "if ssim_scores:\n",
    "    generate_ssim_report(ssim_scores)\n",
    "    rank_simulations(simulation_results)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
